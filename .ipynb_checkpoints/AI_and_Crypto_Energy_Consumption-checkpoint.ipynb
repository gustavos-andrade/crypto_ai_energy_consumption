{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTiALsCpGIEb"
   },
   "source": [
    "\n",
    "\n",
    "# A forecast of the impact of AI and Crypto Minning in Global Energy Consumption\n",
    "\n",
    "\n",
    "A predictive modelling of the future impact of AI and Crypto Mining in the world energy cnsumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbDizIzHGIEd"
   },
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttglU8AmGIEd"
   },
   "source": [
    "* [1. Problem Statement](#0)\n",
    "* [2. Getting Started - Load Libraries and Dataset](#1)\n",
    "    * [2.1. Load Libraries](#1.1)    \n",
    "    * [2.2. Load Dataset](#1.2)\n",
    "* [3. Exploratory Data Analysis](#2)\n",
    "    * [3.1 Descriptive Statistics](#2.1)    \n",
    "    * [3.2. Data Visualisation](#2.2)\n",
    "* [4. Data Preparation](#3)\n",
    "    * [4.1 Data Cleaning](#3.1)\n",
    "    * [4.3.Feature Selection](#3.2)\n",
    "    * [4.3.Data Transformation](#3.3)\n",
    "        * [4.3.1 Rescaling ](#3.3.1)\n",
    "        * [4.3.2 Standardization](#3.3.2)\n",
    "        * [4.3.3 Normalization](#3.3.3)    \n",
    "* [5.Evaluate Algorithms and Models](#4)        \n",
    "    * [5.1. Train/Test Split](#4.1)\n",
    "    * [5.2. Test Options and Evaluation Metrics](#4.2)\n",
    "    * [5.3. Compare Models and Algorithms](#4.3)\n",
    "        * [5.3.1 Common Regression Models](#4.3.1)\n",
    "        * [5.3.2 Ensemble Models](#4.3.2)\n",
    "        * [5.3.3 Deep Learning Models](#4.3.3)  \n",
    "    * [5.4. Time Series based Models-ARIMA and LSTM](#4.4)\n",
    "        * [5.4.1 ARIMA Model](#4.4.1)\n",
    "        * [5.4.2 LSTM Model](#4.4.2)\n",
    "* [6. Model Tuning and Grid Search](#5)\n",
    "    * [6.1 Common Regression, Ensemble and DeepNNRegressor Grid Search](#5.1)\n",
    "    * [6.2 ARIMA and LSTM Grid Search](#5.2)\n",
    "* [7. Finalize the Model](#6)  \n",
    "    * [7.1. Results on test dataset](#6.1)\n",
    "    * [7.1. Variable Intuition/Feature Selection](#6.2)\n",
    "    * [7.3. Save model for later use](#6.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfeG6KXqGIEe"
   },
   "source": [
    "<a id='0'></a>\n",
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktexBSZuGIEe"
   },
   "source": [
    "The goal in this jupyter notebook is to create a pipeline to analyze the problem of enegy consumption prediction in the next 10 years, evaluating the increase in demand by AI and Minning of Crypto currencies.\n",
    "The pipeline is based on the following steps:\n",
    "- Load the data avaliable in csv files and other sources from IEA (International Energy Agency) https://www.iea.org/\n",
    "- Transform the data applying the required formats and merging the datasets\n",
    "- Plot and analyze the statistics to understand patterns and prepare data to training in machine learning models\n",
    "- Apply ensemble methods and tuning of ensemble methods to improve model performance.\n",
    "- The Following Models are implemented and evaluated\n",
    "\n",
    "    * Ada Boost\n",
    "    * Gradient Boosting Method\n",
    "    * Random Forest\n",
    "    * Extra Trees\n",
    "    * Neural Network - Shallow - Using sklearn\n",
    "    * Deep Neural Network - Using Keras\n",
    "- Time Series Models\n",
    "    * ARIMA Model\n",
    "    * LSTM - Using Keras\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOT6Q-2wGIEf"
   },
   "source": [
    "<a id='1'></a>\n",
    "# 2. Getting Started- Loading the data and python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YatF9Tz0GIEf"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Loading the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNauleWk_ixP",
    "outputId": "40218077-5fb0-416b-e901-b89d79d20653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.14.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n",
      "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pandas-datareader) (5.4.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.11/dist-packages (from pandas-datareader) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pandas-datareader) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas-datareader) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas-datareader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas-datareader) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23->pandas-datareader) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pandas-datareader) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas-datareader) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras\n",
    "!pip install --upgrade pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1jGMgGJaGIEg"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_datareader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16988/1944058861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscatter_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_datareader'"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pandas_datareader.data as web\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Libraries for Deep Learning Models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LSTM\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Libraries for Saving the Model\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlzi2rUFGIEh"
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 2.2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KnIMIGp3jg4Z"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_16988/3429415382.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\gusta\\AppData\\Local\\Temp/ipykernel_16988/3429415382.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    world = pd.read_excel('C:\\Users\\gusta\\Documents\\PyCodes\\crypto_ai_energy/World Energy Balances Highlights 2024.xlsx', sheet_name='TimeSeries_1971-2023', header=1)\u001b[0m\n\u001b[1;37m                                                                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "world = pd.read_excel('C:\\Users\\gusta\\Documents\\PyCodes\\crypto_ai_energy/World Energy Balances Highlights 2024.xlsx', sheet_name='TimeSeries_1971-2023', header=1)\n",
    "\n",
    "bitcoin = pd.read_excel('C:\\Users\\gusta\\Documents\\PyCodes\\crypto_ai_energy/Bitcoin_Energy.xlsx')\n",
    "\n",
    "etherium = pd.read_csv('C:\\Users\\gusta\\Documents\\PyCodes\\crypto_ai_energy/Historical Ethereum network annualised electricity consumption (PoS).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SQ_dDwkGIEi"
   },
   "outputs": [],
   "source": [
    "# Transforming the datasets\n",
    "world = world[(world.Country == 'World') & (world.Product == 'Total') & (world.Flow == 'Total final consumption (PJ)')].iloc[:,6:world.shape[1]-1]\n",
    "\n",
    "world = world.T\n",
    "\n",
    "world.reset_index(inplace=True)\n",
    "\n",
    "world.columns = ['Year','Consumption']\n",
    "\n",
    "# Converting the Energy Consumption from PJ to TWh to annalyze the data in the same base of Crypto Minning and Data Centre Energy Consumption\n",
    "world['Consumption'] = pd.to_numeric(world['Consumption']) * 0.27778\n",
    "\n",
    "# Converting the column Date and Time of Bitcoin and Etherium to date\n",
    "bitcoin['Date and Time'] = pd.to_datetime(bitcoin['Date and Time'], errors='coerce')\n",
    "\n",
    "etherium['Date and Time'] = pd.to_datetime(etherium['Date and Time'], errors='coerce')\n",
    "\n",
    "# Conveting Etherium Consumption from GWh to TWh\n",
    "etherium.iloc[:,4:] =  etherium.iloc[:,4:]/1000\n",
    "\n",
    "etherium.columns = etherium.columns.str.split(',').str[0]\n",
    "\n",
    "bitcoin.columns = etherium.columns.str.split(',').str[0]\n",
    "\n",
    "# Merging both crypto datasets\n",
    "crypto = pd.merge(bitcoin[['Date and Time','annualised consumption MAX']], etherium[['Date and Time','annualised consumption GUESS']], on= 'Date and Time',how='left')\n",
    "\n",
    "# Summarising data by year\n",
    "crypto_year = crypto.groupby(crypto['Date and Time'].dt.year)[['annualised consumption MAX','annualised consumption GUESS']].mean().reset_index()\n",
    "\n",
    "crypto_year.fillna(0, inplace=True)\n",
    "\n",
    "crypto_year['Consumption'] = crypto_year['annualised consumption MAX'] + crypto_year['annualised consumption GUESS']\n",
    "\n",
    "crypto_year.drop(columns=['annualised consumption MAX','annualised consumption GUESS'], inplace=True)\n",
    "\n",
    "crypto_year = crypto_year.rename(columns={'Date and Time':'Year'})\n",
    "\n",
    "# Creating the dataframe of Data Center Energy consumption with data provided by IEA\n",
    "datacenter = pd.DataFrame({'Year':list(range(2014,2025)), 'Consumption':[194, 191, 195, 195, 198, 192, 269, 300, 330, 361, 416]})\n",
    "\n",
    "# Merging Crypto and Data Centers in a single dataframe\n",
    "crypto_datacenter = pd.merge(crypto_year, datacenter, on='Year', how='inner')\n",
    "\n",
    "crypto_datacenter['Total Consumption'] = crypto_datacenter['Consumption_x'] +  crypto_datacenter['Consumption_y']\n",
    "\n",
    "crypto_datacenter.drop(columns=['Consumption_x', 'Consumption_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBE-qUdyGIEi"
   },
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QVNFmxZGIEj"
   },
   "source": [
    "<a id='2'></a>\n",
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFhIlOWwGIEk"
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 3.1. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pX-rj5fGIEk"
   },
   "outputs": [],
   "source": [
    "# Crypto and Data Center shape\n",
    "crypto_datacenter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96FMjC8euoP_"
   },
   "outputs": [],
   "source": [
    "# World Energy shape\n",
    "world.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUXPsufmGIEk"
   },
   "outputs": [],
   "source": [
    "# Crypto and Data Center types\n",
    "pd.set_option('display.max_rows', 500)\n",
    "crypto_datacenter.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv6QvjD7vUT3"
   },
   "outputs": [],
   "source": [
    "# World Energy types\n",
    "pd.set_option('display.max_rows', 500)\n",
    "world.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y_kVPiHGIEk"
   },
   "outputs": [],
   "source": [
    "# describe Crypto and Data Center data\n",
    "pd.set_option('display.precision', 3)\n",
    "crypto_datacenter.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d081YMJwJZr"
   },
   "outputs": [],
   "source": [
    "# describe World Energy data\n",
    "pd.set_option('display.precision', 3)\n",
    "world.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOB8B2oaGIEl"
   },
   "source": [
    "<a id='2.2'></a>\n",
    "## 3.2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GGJIsjj7FdX"
   },
   "outputs": [],
   "source": [
    "# Plotting data to visualize\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.merge(world, crypto_datacenter, how = 'left')\n",
    "df.columns = ['Year', 'World', 'Crypto and AI']\n",
    "\n",
    "fig = px.line(df, x='Year', y=['World','Crypto and AI'], labels={'variable':'Source'}, title=\"Total Energy Consumption by Source\",\n",
    "              line_dash_sequence=['solid', 'dash'], height=500, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPzPNKGS_4I-"
   },
   "outputs": [],
   "source": [
    "# Plotting individual charts to analyze the patterns - Crypto and Data Centers Energy Consumption\n",
    "fig = px.line(crypto_datacenter, x='Year', y=['Total Consumption'], color_discrete_sequence=['red'], title=\"Total Energy Consumption by Crypto Minning and Datacenters\", height=500, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSla1DsKBdIM"
   },
   "outputs": [],
   "source": [
    "# Plotting individual charts to analyze the patterns - World Energy Consumption\n",
    "fig = px.line(world, x='Year', y=['Consumption'], color_discrete_sequence=['blue'], title=\"Total World Energy Consumption\", height=500, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNrd6Zx_GIEl"
   },
   "outputs": [],
   "source": [
    "# histograms\n",
    "df.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(10,5))\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrtBNAHhGIEl"
   },
   "outputs": [],
   "source": [
    "# density\n",
    "df.plot(kind='density', subplots=True, layout=(4,4), sharex=False, legend=True, fontsize=1, figsize=(18,15))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygztB9X6GIEl"
   },
   "outputs": [],
   "source": [
    "#Box and Whisker Plots\n",
    "df.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, figsize=(20,15))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHpU_Fq1DPQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-23Jch6GIEm"
   },
   "outputs": [],
   "source": [
    "# correlation\n",
    "correlation = df.corr()\n",
    "pyplot.figure(figsize=(8,8))\n",
    "pyplot.title('Correlation Matrix')\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puMikzehGIEm"
   },
   "outputs": [],
   "source": [
    "# Scatterplot Matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "pyplot.figure(figsize=(12,12))\n",
    "scatter_matrix(df,figsize=(12,12))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl3NdtAVGIEm"
   },
   "source": [
    "<a id='2.3'></a>\n",
    "## 3.3. Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3_LDP4-GIEm"
   },
   "source": [
    "Time series broken down into different time series comonent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpf7QX9PGIEm"
   },
   "outputs": [],
   "source": [
    "Y= crypto_datacenter[\"Total Consumption\"]\n",
    "res = sm.tsa.seasonal_decompose(Y, period=1)\n",
    "fig = res.plot()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(15)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIUK-fKsGIEn"
   },
   "source": [
    "<a id='3'></a>\n",
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVFgCAuyGIEn"
   },
   "source": [
    "<a id='3.1'></a>\n",
    "## 4.1. Data Cleaning\n",
    "Check for the NAs in the rows, either drop them or fill them with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7N98xlHGIEn"
   },
   "outputs": [],
   "source": [
    "#Checking for any null values and removing the null values'''\n",
    "print('Null Values =',crypto_datacenter.isnull().values.any())\n",
    "print('Null Values =',world.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrFcs1K5GIEn"
   },
   "source": [
    "Given that there are null values drop the rown contianing the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRKnHI0UGIEn"
   },
   "outputs": [],
   "source": [
    "# Drop the rows containing NA\n",
    "#dataset.dropna(axis=0)\n",
    "# Fill na with 0\n",
    "#dataset.fillna('0')\n",
    "\n",
    "#Filling the NAs with the mean of the column.\n",
    "#dataset['col'] = dataset['col'].fillna(dataset['col'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "253we6nkGIEn"
   },
   "source": [
    "<a id='3.3'></a>\n",
    "## 4.3. Spliting train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsN4bMbjGIEo"
   },
   "outputs": [],
   "source": [
    "Ycd= crypto_datacenter['Total Consumption']\n",
    "Yw = world['Consumption']\n",
    "Xcd = crypto_datacenter.loc[:,crypto_datacenter.columns != 'Total Consumption']\n",
    "Xw = world.loc[:,world.columns != 'Consumption']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRjxPEfXGIEu"
   },
   "source": [
    "<a id='3.4'></a>\n",
    "## 4.4. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5PyfvtlGIEu"
   },
   "source": [
    "<a id='3.4.1'></a>\n",
    "### 4.4.1. Rescale Data\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefit from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwFjGSUzGIEu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledXcd = pd.DataFrame(scaler.fit_transform(Xcd))\n",
    "rescaledXw = pd.DataFrame(scaler.fit_transform(Xw))\n",
    "# summarize transformed data\n",
    "rescaledXcd.head(5)\n",
    "rescaledXw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TrdCmxKGIEv"
   },
   "source": [
    "<a id='3.4.2'></a>\n",
    "### 4.4.2. Standardize Data\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c1MgbiDGIEv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_cd = StandardScaler().fit(Xcd)\n",
    "scaler_w = StandardScaler().fit(Xw)\n",
    "StandardisedX_cd = pd.DataFrame(scaler_cd.fit_transform(Xcd))\n",
    "StandardisedX_w = pd.DataFrame(scaler_w.fit_transform(Xw))\n",
    "# summarize transformed data\n",
    "StandardisedX_cd.head(5)\n",
    "StandardisedX_w.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTMVqBwKGIEv"
   },
   "source": [
    "<a id='3.4.3'></a>\n",
    "### 4.4.1. Normalize Data\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5UyiH8lGIEv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler_cd = Normalizer().fit(Xcd)\n",
    "scaler_w = Normalizer().fit(Xw)\n",
    "NormalizedX_cd = pd.DataFrame(scaler_cd.fit_transform(Xcd))\n",
    "NormalizedX_w = pd.DataFrame(scaler_w.fit_transform(Xw))\n",
    "# summarize transformed data\n",
    "NormalizedX_cd.head(5)\n",
    "NormalizedX_w.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rO8K5S6GIEv"
   },
   "source": [
    "<a id='4'></a>\n",
    "# 5. Evaluate Algorithms and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PT8GYc2GIEw"
   },
   "source": [
    "<a id='4.1'></a>\n",
    "## 5.1. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGOsUqrXGIEw"
   },
   "outputs": [],
   "source": [
    "# split out validation dataset for the end\n",
    "\n",
    "validation_size = 0.2\n",
    "\n",
    "#In case the data is not dependent on the time series, then train and test split randomly\n",
    "seed = 7\n",
    "# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n",
    "#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n",
    "\n",
    "train_size_cd = int(len(Xcd) * (1-validation_size))\n",
    "train_size_w = int(len(Xw) * (1-validation_size))\n",
    "X_train_cd, X_validation_cd = Xcd[0:train_size_cd], Xcd[train_size_cd:len(Xcd)]\n",
    "X_train_w, X_validation_w = Xw[0:train_size_w], Xw[train_size_w:len(Xw)]\n",
    "Y_train_w, Y_validation_w = Yw[0:train_size_w], Yw[train_size_w:len(Xw)]\n",
    "Y_train_cd, Y_validation_cd = Ycd[0:train_size_cd], Ycd[train_size_cd:len(Xcd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beMUbDgFGIEw"
   },
   "source": [
    "<a id='4.2'></a>\n",
    "## 5.2. Test Options and Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHs2HYuIGIEw"
   },
   "outputs": [],
   "source": [
    "# test options for regression\n",
    "num_folds = 8\n",
    "scoring = 'neg_mean_squared_error'\n",
    "#scoring ='neg_mean_absolute_error'\n",
    "#scoring = 'r2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRmoSNq-GIEx"
   },
   "source": [
    "<a id='4.3'></a>\n",
    "## 5.3. Compare Models and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeHsTD8dGIEx"
   },
   "source": [
    "<a id='4.3.1'></a>\n",
    "### 5.3.1. Common Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF_MiehQGIEy"
   },
   "outputs": [],
   "source": [
    "# spot check the algorithms\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVR', SVR()))\n",
    "#Neural Network\n",
    "#models.append(('MLP', MLPRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQkzqVE2GIEy"
   },
   "source": [
    "<a id='4.3.2'></a>\n",
    "### 5.3.2. Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbUvQgj3GIEy"
   },
   "outputs": [],
   "source": [
    "#Ensable Models\n",
    "# Boosting methods\n",
    "models.append(('ABR', AdaBoostRegressor()))\n",
    "models.append(('GBR', GradientBoostingRegressor()))\n",
    "# Bagging methods\n",
    "models.append(('RFR', RandomForestRegressor()))\n",
    "models.append(('ETR', ExtraTreesRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRP1jC-lGIEy"
   },
   "source": [
    "<a id='4.3.3'></a>\n",
    "### 5.3.3. Deep Learning Model-NN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj1O8ZYSGIEz"
   },
   "outputs": [],
   "source": [
    "#Running deep learning models and performing cross validation takes time\n",
    "#Set the following Flag to 0 if the Deep LEarning Models Flag has to be disabled\n",
    "EnableDeepLearningRegreesorFlag = 0\n",
    "\n",
    "def create_model(neurons=12, activation='relu', learn_rate = 0.01, momentum=0):\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
    "        #The number of hidden layers can be increased\n",
    "        model.add(Dense(2, activation=activation))\n",
    "        # Final output layer\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        # Compile model\n",
    "        optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6fvdqKqGIEz"
   },
   "outputs": [],
   "source": [
    "#Add Deep Learning Regressor\n",
    "if ( EnableDeepLearningRegreesorFlag == 1):\n",
    "    models.append(('DNN', KerasRegressor(build_fn=create_model, epochs=100, batch_size=100, verbose=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex3TDhwDGIEz"
   },
   "source": [
    "### K-folds cross validation - Crypto and AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DqCiuY2GIEz"
   },
   "outputs": [],
   "source": [
    "results_cd = []\n",
    "names_cd = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    #converted mean square error to positive. The lower the beter\n",
    "    cv_results_cd = -1* cross_val_score(model, X_train_cd, Y_train_cd, cv=kfold, scoring=scoring)\n",
    "    results_cd.append(cv_results_cd)\n",
    "    names_cd.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results_cd.mean(), cv_results_cd.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qHx6n_KX6zZ"
   },
   "source": [
    "### K-folds cross validation - World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYc1KUTAX6zb"
   },
   "outputs": [],
   "source": [
    "results_w = []\n",
    "names_w = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    #converted mean square error to positive. The lower the beter\n",
    "    cv_results_w = -1* cross_val_score(model, X_train_w, Y_train_w, cv=kfold, scoring=scoring)\n",
    "    results_w.append(cv_results_w)\n",
    "    names_w.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results_w.mean(), cv_results_w.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1f1ci-HGIE0"
   },
   "source": [
    "### Algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdpVJMAtGIE0"
   },
   "outputs": [],
   "source": [
    "# compare algorithms - Crypto and AI\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison - Crypto and AI')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results_cd)\n",
    "ax.set_xticklabels(names_cd)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00K_vEv3GIE0"
   },
   "source": [
    "The chart shows MSE. Lower the MSE, better is the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzCx0PlUG9MX"
   },
   "outputs": [],
   "source": [
    "# compare algorithms - World\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison - World')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results_w)\n",
    "ax.set_xticklabels(names_w)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by3YNzAnGIE0"
   },
   "source": [
    "<a id='4.4'></a>\n",
    "## 5.4. Time Series based Models- ARIMA and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHY81AwxGIE0"
   },
   "source": [
    "<a id='4.4.1'></a>\n",
    "### 5.4.1 Time Series Model - ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdBtyhOuGIE0"
   },
   "outputs": [],
   "source": [
    "#Preparing data for the ARIMAX Model, seperating endogeneous and exogenous variables\n",
    "# Crypto and AI\n",
    "X_train_ARIMA_cd = X_train_cd\n",
    "X_validation_ARIMA_cd =X_validation_cd\n",
    "tr_len_cd = len(X_train_ARIMA_cd)\n",
    "te_len_cd = len(X_validation_ARIMA_cd)\n",
    "to_len_cd = len(Xcd)\n",
    "#World\n",
    "X_train_ARIMA_w = X_train_w\n",
    "X_validation_ARIMA_w = X_validation_w\n",
    "tr_len_w = len(X_train_ARIMA_w)\n",
    "te_len_w = len(X_validation_ARIMA_w)\n",
    "to_len_w = len(Xw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qmE096jug6H"
   },
   "outputs": [],
   "source": [
    "# ARIMA Crypto and AI\n",
    "from statsmodels.tsa.arima.model import ARIMA # Corrected import path\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX # Keep this if you need SARIMAX later\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "modelARIMA_cd = ARIMA(endog = Y_train_cd, exog = X_train_ARIMA_cd, order=(1,0,0)) # Note: order is a tuple\n",
    "# modelARIMA= SARIMAX(Y_train,order=(1,1,0),seasonal_order=(1,0,0,0),exog = X_train_ARIMA)\n",
    "\n",
    "model_fit_cd = modelARIMA_cd.fit()\n",
    "\n",
    "print(model_fit_cd.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7nE1g8gGIE1"
   },
   "outputs": [],
   "source": [
    "error_Training_ARIMA_cd = mean_squared_error(Y_train_cd, model_fit_cd.fittedvalues)\n",
    "predicted_cd = model_fit_cd.predict(start = tr_len_cd -1 ,end = to_len_cd -1, exog = X_validation_ARIMA_cd)[1:]\n",
    "error_Test_ARIMA_cd = mean_squared_error(Y_validation_cd, predicted_cd)\n",
    "error_Test_ARIMA_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmoTo4_oHvoj"
   },
   "outputs": [],
   "source": [
    "# ARIMA World\n",
    "from statsmodels.tsa.arima.model import ARIMA # Corrected import path\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX # Keep this if you need SARIMAX later\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "modelARIMA_w = ARIMA(endog=Y_train_w, exog=X_train_ARIMA_w, order=(1,0,0)) # Note: order is a tuple\n",
    "# modelARIMA= SARIMAX(Y_train,order=(1,1,0),seasonal_order=(1,0,0,0),exog = X_train_ARIMA)\n",
    "\n",
    "model_fit_w = modelARIMA_w.fit()\n",
    "\n",
    "print(model_fit_w.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqSffaGpHzGc"
   },
   "outputs": [],
   "source": [
    "error_Training_ARIMA_w = mean_squared_error(Y_train_w, model_fit_w.fittedvalues)\n",
    "predicted_w = model_fit_w.predict(start = tr_len_w -1 ,end = to_len_w -1, exog = X_validation_ARIMA_w)[1:]\n",
    "error_Test_ARIMA_w = mean_squared_error(Y_validation_w,predicted_w)\n",
    "error_Test_ARIMA_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7jZSawIGIE1"
   },
   "outputs": [],
   "source": [
    "#Add Cross validation if possible\n",
    "# #model = build_model(_alpha=1.0, _l1_ratio=0.3)\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "# scores = cross_val_score(modelARIMA, X_train, Y_train, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLxrosieGIE1"
   },
   "source": [
    "<a id='4.4.2'></a>\n",
    "### 5.4.2 LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXjFtdThGIE2"
   },
   "source": [
    "The data needs to be in 3D format for the LSTM model. So, Performing the data transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3aFlkvXGIE2"
   },
   "outputs": [],
   "source": [
    "# LSTM training - Crypto and AI\n",
    "X_train_LSTM_cd, X_validation_LSTM_cd = np.array(X_train_cd), np.array(X_validation_cd)\n",
    "Y_train_LSTM_cd, Y_validation_LSTM_cd = np.array(Y_train_cd), np.array(Y_validation_cd)\n",
    "X_train_LSTM_cd = X_train_LSTM_cd.reshape((X_train_LSTM_cd.shape[0], 1, X_train_LSTM_cd.shape[1]))\n",
    "X_validation_LSTM_cd = X_validation_LSTM_cd.reshape((X_validation_LSTM_cd.shape[0], 1, X_validation_LSTM_cd.shape[1]))\n",
    "print(X_train_LSTM_cd.shape, Y_train_LSTM_cd.shape, X_validation_LSTM_cd.shape, Y_validation_LSTM_cd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4afFA7LseEyP"
   },
   "outputs": [],
   "source": [
    "# LSTM training - World\n",
    "X_train_LSTM_w, X_validation_LSTM_w = np.array(X_train_w), np.array(X_validation_w)\n",
    "Y_train_LSTM_w, Y_validation_LSTM_w = np.array(Y_train_w), np.array(Y_validation_w)\n",
    "X_train_LSTM_w = X_train_LSTM_w.reshape((X_train_LSTM_w.shape[0], 1, X_train_LSTM_w.shape[1]))\n",
    "X_validation_LSTM_w = X_validation_LSTM_w.reshape((X_validation_LSTM_w.shape[0], 1, X_validation_LSTM_w.shape[1]))\n",
    "print(X_train_LSTM_w.shape, Y_train_LSTM_w.shape, X_validation_LSTM_w.shape, Y_validation_LSTM_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzAoQa0UGIE2"
   },
   "outputs": [],
   "source": [
    "# design network - Crypto and AI\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def create_LSTMmodel(neurons=12, learn_rate = 0.01, momentum=0.1):\n",
    "        # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train_LSTM_cd.shape[1], X_train_LSTM_cd.shape[2])))\n",
    "    #More number of cells can be added if needed\n",
    "    model.add(Dense(1))\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=0.1)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "LSTMModel_cd = create_LSTMmodel(12, learn_rate = 0.01, momentum=0)\n",
    "LSTMModel_fit_cd = LSTMModel_cd.fit(X_train_LSTM_cd, Y_train_LSTM_cd, validation_data=(X_validation_LSTM_cd, Y_validation_LSTM_cd),epochs=50, batch_size=72, verbose=0, shuffle=False)# plot history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ibp0T8p5fNO4"
   },
   "outputs": [],
   "source": [
    "# design network - World\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def create_LSTMmodel(neurons=12, learn_rate = 0.01, momentum=0.1):\n",
    "        # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train_LSTM_w.shape[1], X_train_LSTM_w.shape[2])))\n",
    "    #More number of cells can be added if needed\n",
    "    model.add(Dense(1))\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=0.1)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "LSTMModel_w = create_LSTMmodel(12, learn_rate = 0.01, momentum=0)\n",
    "LSTMModel_fit_w = LSTMModel_w.fit(X_train_LSTM_w, Y_train_LSTM_w, validation_data=(X_validation_LSTM_w, Y_validation_LSTM_w),epochs=50, batch_size=72, verbose=0, shuffle=False)# plot history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlfButowGIE2"
   },
   "outputs": [],
   "source": [
    "#Visual plot to check if the error is reducing- Crypto and AI\n",
    "pyplot.plot(LSTMModel_fit_cd.history['loss'], label='train')\n",
    "pyplot.plot(LSTMModel_fit_cd.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iu4vmSmGIE2"
   },
   "outputs": [],
   "source": [
    "error_Training_LSTM_cd = mean_squared_error(Y_train_LSTM_cd, LSTMModel_cd.predict(X_train_LSTM_cd))\n",
    "predicted_cd = LSTMModel_cd.predict(X_validation_LSTM_cd)\n",
    "error_Test_LSTM_cd = mean_squared_error(Y_validation_cd, predicted_cd)\n",
    "error_Test_LSTM_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuePpYRggjag"
   },
   "outputs": [],
   "source": [
    "#Visual plot to check if the error is reducing- World\n",
    "pyplot.plot(LSTMModel_fit_w.history['loss'], label='train')\n",
    "pyplot.plot(LSTMModel_fit_w.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPCrDHA5gjag"
   },
   "outputs": [],
   "source": [
    "error_Training_LSTM_w = mean_squared_error(Y_train_LSTM_w, LSTMModel_cd.predict(X_train_LSTM_w))\n",
    "predicted_w = LSTMModel_w.predict(X_validation_LSTM_w)\n",
    "error_Test_LSTM_w = mean_squared_error(Y_validation_w, predicted_w)\n",
    "error_Test_LSTM_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpNwOJIdGIE3"
   },
   "source": [
    "### Overall Comparison of all the algorithms ( including Time Series Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4xsbT9GGIE3"
   },
   "outputs": [],
   "source": [
    "# compare algorithms - Crypto and AI\n",
    "results_cd.append(error_Test_ARIMA_cd)\n",
    "names_cd.append(\"ARIMA\")\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison-Post Time Series - Crypto and AI')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results_cd)\n",
    "ax.set_xticklabels(names_cd)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6tGn4sAGIE3"
   },
   "source": [
    "Grid Search uses Cross validation which isn't appropriate for the time series models such as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD6gmjk7g3lB"
   },
   "outputs": [],
   "source": [
    "# compare algorithms - World\n",
    "results_w.append(error_Test_ARIMA_w)\n",
    "names_w.append(\"ARIMA\")\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison-Post Time Series')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results_w)\n",
    "ax.set_xticklabels(names_w)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEvHqmZSGIE3"
   },
   "source": [
    "<a id='5'></a>\n",
    "# 6. Model Tuning and Grid Search\n",
    "This section shown the Grid search for all the Machine Learning and time series models mentioned in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1R5dQTPGIE4"
   },
   "source": [
    "<a id='5.1'></a>\n",
    "### 6.1. Common Regression, Ensemble and DeepNNRegressor Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b5kRMOOqBqq"
   },
   "source": [
    "Grid Search Crypto and AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHSAO7pyGIE6"
   },
   "outputs": [],
   "source": [
    "# 8. Grid search : RandomForestRegressor - Crypto and AI\n",
    "'''\n",
    "n_estimators : integer, optional (default=10)\n",
    "    The number of trees in the forest.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = RandomForestRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train_cd, Y_train_cd)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja44zastGIE6"
   },
   "outputs": [],
   "source": [
    "# 10. Grid search : ExtraTreesRegressor - Crypto and AI\n",
    "'''\n",
    "n_estimators : integer, optional (default=10)\n",
    "    The number of trees in the forest.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = ExtraTreesRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train_cd, Y_train_cd)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_Gv7MW7pId7"
   },
   "outputs": [],
   "source": [
    "# 11. Grid search : AdaBoostRegre - Crypto and AI\n",
    "'''\n",
    "n_estimators : integer, optional (default=50)\n",
    "    The maximum number of estimators at which boosting is terminated.\n",
    "    In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "learning_rate : float, optional (default=1.)\n",
    "    Learning rate shrinks the contribution of each regressor by\n",
    "    ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "    ``n_estimators``.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400],\n",
    "             'learning_rate': [1, 2, 3]}\n",
    "model = AdaBoostRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train_cd, Y_train_cd)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyEPqaEep7jk"
   },
   "source": [
    "Grid Search World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAhLOYT9pId6"
   },
   "outputs": [],
   "source": [
    "# 8. Grid search : RandomForestRegressor - World\n",
    "'''\n",
    "n_estimators : integer, optional (default=10)\n",
    "    The number of trees in the forest.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = RandomForestRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train_w, Y_train_w)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxpB1WOzpId7"
   },
   "outputs": [],
   "source": [
    "# 10. Grid search : ExtraTreesRegressor - World\n",
    "'''\n",
    "n_estimators : integer, optional (default=10)\n",
    "    The number of trees in the forest.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = ExtraTreesRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train_w, Y_train_w)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snN4hEJ0GIE7"
   },
   "source": [
    "<a id='5.2'></a>\n",
    "### 6.2. Grid Search- Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6k-X7fFhGIE7"
   },
   "outputs": [],
   "source": [
    "#Grid Search for ARIMA Model - Crypto and AI\n",
    "#Change p,d and q and check for the best result\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "#Assuming that the train and Test Data is already defined before\n",
    "def evaluate_arima_model(arima_order):\n",
    "    #predicted = list()\n",
    "    modelARIMA_cd = ARIMA(endog=Y_train_cd,exog=X_train_ARIMA_cd,order=arima_order)\n",
    "    model_fit_cd = modelARIMA_cd.fit()\n",
    "    #error on the test set\n",
    "#     tr_len = len(X_train_ARIMA)\n",
    "#     to_len = len(X_train_ARIMA) + len(X_validation_ARIMA)\n",
    "#     predicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_validation_ARIMA)[1:]\n",
    "#     error = mean_squared_error(predicted, Y_validation)\n",
    "    # error on the training set\n",
    "    error = mean_squared_error(Y_train_cd, model_fit_cd.fittedvalues)\n",
    "    return error\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.7f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))\n",
    "\n",
    "# evaluate parameters\n",
    "p_values = [0, 1, 2, 3]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_PEaYccq_xA"
   },
   "outputs": [],
   "source": [
    "#Grid Search for ARIMA Model - World\n",
    "#Change p,d and q and check for the best result\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "#Assuming that the train and Test Data is already defined before\n",
    "def evaluate_arima_model(arima_order):\n",
    "    #predicted = list()\n",
    "    modelARIMA=ARIMA(endog=Y_train_w,exog=X_train_ARIMA_w,order=arima_order)\n",
    "    model_fit = modelARIMA.fit()\n",
    "    #error on the test set\n",
    "#     tr_len = len(X_train_ARIMA)\n",
    "#     to_len = len(X_train_ARIMA) + len(X_validation_ARIMA)\n",
    "#     predicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_validation_ARIMA)[1:]\n",
    "#     error = mean_squared_error(predicted, Y_validation)\n",
    "    # error on the training set\n",
    "    error = mean_squared_error(Y_train_w, model_fit.fittedvalues)\n",
    "    return error\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.7f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))\n",
    "\n",
    "# evaluate parameters\n",
    "p_values = [0, 1, 2, 3]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4XC28vDGIE7"
   },
   "source": [
    "<a id='6'></a>\n",
    "# 7. Finalise the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDufp8SyGIE7"
   },
   "source": [
    "Let us select one of the model to finalize the data. Looking at the results for the Random Forest Model. Looking at the results for the RandomForestRegressor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBwkPfNyGIE8"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1. Results on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGTRDZrNGIE8"
   },
   "outputs": [],
   "source": [
    "# Fit model - Crypto and AI\n",
    "# prepare model\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "#rescaledX = scaler.transform(X_train)\n",
    "model_cd_et = ExtraTreesRegressor(n_estimators=50) # rbf is default kernel\n",
    "model_cd_et.fit(X_train_cd, Y_train_cd)\n",
    "\n",
    "# ARIMA\n",
    "modelARIMA_cd = ARIMA(endog = Y_train_cd, exog = X_train_ARIMA_cd, order=(3,0,1)) # Note: order is a tuple\n",
    "model_fit_ARIMA_cd = modelARIMA_cd.fit()\n",
    "# Fitting the ARIMA model\n",
    "error_Training_ARIMA_cd = mean_squared_error(Y_train_cd, model_fit_ARIMA_cd.fittedvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RGTawpIJjmM"
   },
   "outputs": [],
   "source": [
    "# estimate accuracy on validation set - Crypto and AI\n",
    "# transform the validation dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "#rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions_et_cd = model_cd_et.predict(X_validation_cd)\n",
    "print('-'*50)\n",
    "print('EXTRA TREES MODEL')\n",
    "print('-'*50)\n",
    "print('Mean Squared Error:',mean_squared_error(Y_validation_cd, predictions_et_cd))\n",
    "print('r2 Score:', r2_score(Y_validation_cd, predictions_et_cd))\n",
    "\n",
    "print('-'*50)\n",
    "print('ARIMA MODEL')\n",
    "print('-'*50)\n",
    "predictions_arima_cd = model_fit_ARIMA_cd.predict(start = tr_len_cd - 1, end = to_len_cd - 1, exog = X_validation_ARIMA_cd)[1:]\n",
    "print('Mean Squared Error:',mean_squared_error(Y_validation_cd, predictions_arima_cd))\n",
    "print('r2 Score:', r2_score(Y_validation_cd, predictions_arima_cd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU4S8qLSG1Df"
   },
   "outputs": [],
   "source": [
    "# Fit model - World\n",
    "# prepare model\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "#rescaledX = scaler.transform(X_train)\n",
    "model_w_et = ExtraTreesRegressor(n_estimators=50) # rbf is default kernel\n",
    "model_w_et.fit(X_train_w, Y_train_w)\n",
    "\n",
    "modelARIMA_w = ARIMA(endog = Y_train_w, exog = X_train_ARIMA_w, order=(3,0,2)) # Note: order is a tuple\n",
    "model_fit_ARIMA_w = modelARIMA_w.fit()\n",
    "# Fitting the ARIMA model\n",
    "error_Training_ARIMA_cd = mean_squared_error(Y_train_cd, model_fit_ARIMA_cd.fittedvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOGphrkaGIE8"
   },
   "outputs": [],
   "source": [
    "# estimate accuracy on validation set - World\n",
    "# transform the validation dataset\n",
    "#rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions_et_w = model_cd_et.predict(X_validation_w)\n",
    "print('-'*50)\n",
    "print('EXTRA TREES MODEL')\n",
    "print('-'*50)\n",
    "print('Mean Squared Error:',mean_squared_error(Y_validation_w, predictions_et_w))\n",
    "print('r2 Score:', r2_score(Y_validation_w, predictions_et_w))\n",
    "\n",
    "print('-'*50)\n",
    "print('ARIMA MODEL')\n",
    "print('-'*50)\n",
    "predictions_arima_w = model_fit_ARIMA_w.predict(start = tr_len_w - 1, end = to_len_w - 1, exog = X_validation_ARIMA_w)[1:]\n",
    "print('Mean Squared Error:',mean_squared_error(Y_validation_w, predictions_arima_w))\n",
    "print('r2 Score:', r2_score(Y_validation_w, predictions_arima_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBsDS_1eznAV"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(X_validation_ARIMA_w, Y_validation_w, left_index=True, right_index=True)\n",
    "df_f = pd.merge(df, predictions_arima_w, left_index=True, right_index=True)\n",
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7g-mKXF2Rzr"
   },
   "outputs": [],
   "source": [
    "model_fit_ARIMA_w.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LkRRkZQwM5B"
   },
   "outputs": [],
   "source": [
    "fig = px.line(df_f, x= 'Year', y=['Consumption','predicted_mean'], color_discrete_sequence=['blue', 'red'], title=\"Total World Energy Consumption\", height=500, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyk53FVQqVpP"
   },
   "outputs": [],
   "source": [
    "df2 = pd.merge(X_validation_ARIMA_cd, Y_validation_cd, left_index=True, right_index=True)\n",
    "df2_f = pd.merge(df2, predictions_arima_cd, left_index=True, right_index=True)\n",
    "fig = px.line(df2_f, x= 'Year', y=['Total Consumption','predicted_mean'], color_discrete_sequence=['blue', 'red'], title=\"Total World Energy Consumption\", height=500, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pThMMXoMGIE8"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2. Variable Intuition/Feature Importance\n",
    "Let us look into the Feature Importance of the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCvYaig8GIE9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train,Y_train)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based regressors\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVixAtLUGIE9"
   },
   "source": [
    "<a id='6.3'></a>\n",
    "## 7.3. Save Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GNI7U7GGIE9"
   },
   "outputs": [],
   "source": [
    "# Save Model Using Pickle\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhvunQbKGIE9"
   },
   "outputs": [],
   "source": [
    "# some time later...\n",
    "# load the model from disk\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "# estimate accuracy on validation set\n",
    "#rescaledValidationX = scaler.transform(X_validation) #in case the data is scaled.\n",
    "#predictions = model.predict(rescaledValidationX)\n",
    "predictions = model.predict(X_validation)\n",
    "result = mean_squared_error(Y_validation, predictions)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 206,
  "_is_fork": false,
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
